# ml-training-yandex

Репозиторий с моими решениями тренировок по ML от Яндекса

### [Тренировки по классическому ML](https://yandex.ru/yaintern/training/ml-training-2023)

#### Скучные числа

1. [Контест](https://contest.yandex.ru/contest/53019/enter), [Решение](./classic/hw1/), Результат: [1/1, 1/1]
2. [Контест](https://contest.yandex.ru/contest/53020/enter), [Решение](./classic/hw2/), Результат: [1/1, 1/1]
3. [Контест](https://contest.yandex.ru/contest/53021/enter), [Решение](./classic/hw3/), Результат: [1/1, 1/1, 1/1]
4. [Контест](https://contest.yandex.ru/contest/53022/enter), [Решение](./classic/hw4/), Результат: [0.35/1, 0/1] - *Хотя я считаю, что решил всё правильно. А может и нет* ¯\\\_(ツ)\_/¯
5. [Контест](https://contest.yandex.ru/contest/56809/problems/A/), [Решение](./classic/final/), Результат: [1.36/1.5, 1.23/1.5] - **TODO:** *Вернуться через неделю и попробовать улучшить результат*


#### Весёлые выводы
Самое интересное, конечно, чему ж я научился?

1. Я наконец-то понял всю красу метода Наивного Байеса. Кажется, что это очень полезно в биологии для классификации, потому что там практически любые величины подчиняются уже известным распределениям. Интересно, можно ли не делать предположений о независимости признаков? Нужно ли тут хорошое понимание предметной области, чтобы делать предположения о совместных распределениях, или можно придумать универсальный алгоритм обучения моделей? Для дальнейшего изучения уже заприметил курс от МФТИ: [часть 1](https://youtube.com/playlist?list=PLk4h7dmY2eYGrzXC5o9vqxDrZ_6P6QQwA&si=8-M0xgFb2DUX_UB6), [часть 2](https://youtube.com/playlist?list=PLk4h7dmY2eYHu8wvU7tEkFK8pFiQYuRWV&si=ffYPTQQPS_gVJjUS)
2. Повторил ансамблирование. Большая и важная тема, за раз выучить не получится. Вспомнил обоснование выбора базовых моделей для бэггинга и градиентного бустинга, вспомнил, чем градиентный бустинг похож на градиентный спуск и в целом пересмотрел лекции, перечитал хендбуки
3. Реализовал KNN. Всегда пропускал наивную реализацию, потому что считал, что это слишком просто, но написать векторизированный вариант оказалось не так уж и очевидно. Узнал про прикольный метод `np.einsum()`
4. Вспомнил про B-сплайны (в ходе решения одной из домашек, но какой - не помню), которые мельком упомянул наш преподаватель по матану в конце курса. Почитал про них, реализовал, кайфанул, что линейную модель можно сделать нелинейной таким прикольным способом, взял на вооружение
5. Узнал про Shapley Values и то, как с их помощью интерпретировать модели. Нашёл прикольный аналог хендбука от яндекса, в котором, в том числе, есть раздел про [значения Шепли](https://deepmachinelearning.ru/docs/Machine-learning/Complex-models-interpretation/Shapley-values). Я так и не понял, почему проверяющая система не зачла моё решение, зато мне пришлось сто раз перепроверить все формулы, из-за чего в голове эта информация останется на долго =)
6. И много-много других маленьких приёмчиков, тонкостей и открытий



### [Тренировки по NLP](https://yandex.ru/yaintern/training/ml-training-october-2024)

**soon...**